{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CherryMunch Prototype\n",
    "\n",
    "Take A DOI, get the page, then collect the relevant data from the page, save and leave\n",
    "WITHOUT interacting with the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def get_paths(publisher,conn,doi):\n",
    "    ###to do, build database with all of these\n",
    "    cur = ca.find({'pub_website':publisher})\n",
    "    if cur.count()==0:\n",
    "        raise Exception('no publisher website for '+ doi) \n",
    "    return cur\n",
    "\n",
    "def get_publisher(response):\n",
    "    url = response.url\n",
    "    publisher = url.split('/')[2]\n",
    "    return publisher\n",
    "\n",
    "mongo_url = 'mongodb://localhost:6666/'\n",
    "db = 'Cherry'\n",
    "coll = 'CherryMunch'\n",
    "client = MongoClient(mongo_url)\n",
    "ca = client[db][coll]\n",
    "\n",
    "doi = '10.1515/mgmc-2012-0907'\n",
    "target_stub = 'http://dx.doi.org/'\n",
    "target = target_stub + doi\n",
    "r = requests.get(target)\n",
    "response=TextResponse(r.url,body=r.text, encoding='utf-8')\n",
    "###SENSE Who's publisher this is\n",
    "pub = get_publisher(response)\n",
    "###LOAD CORRECT XPATHS:\n",
    "#paths_list = get_paths(pub,ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record1 = {\n",
    "        'publisher':'Wiley',\n",
    "        'pub_website':'onlinelibrary.wiley.com',\n",
    "        'x_title':'string(//*[@class=\"article-header__title\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div/p)',\n",
    "        'x_people':'//*[@class=\"article-header__authors-item\"]',\n",
    "        'x_depts':'//*[@class=\"article-header__authors-item\"]',\n",
    "        'x_person':'string(.//*[@class=\"article-header__authors-name\"])',\n",
    "        'x_dept':'string(.//*[@class=\"article-header__authors-item-aff-addr\"])',\n",
    "        'x_date':'string(//time[@id=\"first-published-date\"])',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date,'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record2 = {\n",
    "        'publisher':'Wiley',\n",
    "        'pub_website':'onlinelibrary.wiley.com',\n",
    "        'x_title':'string(//*[@class=\"articleTitle\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div[@class=\"para\"])',\n",
    "        'x_people':'//*[@id=\"authors\"]/li',\n",
    "        'x_depts':'//*[@id=\"authorsAffiliations\"]/li',\n",
    "        'x_person':'text()',\n",
    "        'x_dept':'p/text()',\n",
    "        'x_date':'string(//p[@id=\"publishedOnlineDate\"])',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(u' '.join(date.split(' ')[-3:]),'%d %b %Y')\n",
    "        ''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "pexl=[]\n",
    "for p in pex:\n",
    "    if type(p)==list:\n",
    "        pexl+=p\n",
    "    else:\n",
    "        pexl.append(p)\n",
    "for p in pexl:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    p = re.sub(' and','',p )\n",
    "    words = p.split(\" \")\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=filter(lambda x: x!=u' ',pex2)\n",
    "'''\n",
    "    }\n",
    "\n",
    "record3 = {\n",
    "        'publisher':'ACS',\n",
    "        'pub_website':'pubs.acs.org',\n",
    "        'x_title':'string(//*[@class=\"articleTitle\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstractBox\"])',\n",
    "        'x_people':'//*[@id=\"authors\"]/span',\n",
    "        'x_person':'string(span[1])',\n",
    "        'x_depts':'//*[@class=\"affiliations\"]/div',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'string(//*[@id=\"pubDate\"])',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(' '.join([sp[-3]]+[sp[-2][:-1]]+[sp[-1]]),'%B %d %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record4 = { 'publisher':'American Physical Society',\n",
    "        'pub_website':'journals.aps.org',\n",
    "        'x_title':'//div[@id=\"title\"]/descendant::h3/text()',\n",
    "        'x_abstract':'//meta[@name=\"description\"]/@content',\n",
    "        'x_people':'//section[@class=\"article authors open\"]/div/p',\n",
    "        'x_person':'*[1]/text()',\n",
    "        'x_depts':'//section[@class=\"article authors open\"]/div/ul[@class=\"no-bullet\"]',\n",
    "        'x_dept':'li/text()',\n",
    "        'x_date':'//ul[@class=\"inline-list pub-dates\"]/li/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(' '.join(sp[1:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record5 = { 'publisher':'Springer',\n",
    "        'pub_website':'link.springer.com',\n",
    "        'x_title':'//*[@class=\"ArticleTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"Abs1\"]/p)',\n",
    "        'x_people':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_person':'*/span[@class=\"AuthorName\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_dept':'descendant::span[@class=\"AuthorsName_affiliation\"]/span/text()',\n",
    "        'x_date':'//*[@class=\"ArticleCitation_Year\"]/time/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(date,'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record6 = { 'publisher':'Springer',\n",
    "        'pub_website':'link.springer.com',\n",
    "        'x_title':'//*[@class=\"ArticleTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"Abs1\"]/p)',\n",
    "        'x_people':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_person':'*/span[@class=\"AuthorName\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_dept':'descendant::span[@class=\"AuthorsName_affiliation\"]/span/text()',\n",
    "        'x_date':'//*[@class=\"ArticleCitation_Year\"]/time/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(date,'%B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record7 = { 'publisher':'ScienceDirect',\n",
    "        'pub_website':'www.sciencedirect.com',\n",
    "        'x_title':'//*[@class=\"svTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@class=\"abstract svAbstract \"])',\n",
    "        'x_people':'//ul[@class=\"authorGroup noCollab svAuthor\"]/li',\n",
    "        'x_person':'*[@class=\"authorName svAuthor\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"affiliation authAffil\"]/li',\n",
    "        'x_dept':'span/text()',\n",
    "        'x_date':'//dl[@class=\"articleDates\"]/dd/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(\" \".join(sp[-3:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record8 = { 'publisher':'International Union of Crystallography',\n",
    "        'pub_website':'scripts.iucr.org',\n",
    "        'x_title':'string(//*[@class=\"ica_title\"])',\n",
    "        'x_abstract':'string(//*[@class=\"ica_abstract\"])',\n",
    "        'x_people':'//*[@class=\"ica_authors\"]/a',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//none-here',#no affiliations given on page\n",
    "        'x_dept':'//none-here',\n",
    "        'x_date':'//div[@class=\"ica_header\"]/span[2]/text()',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date[2:6],'%Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record9 = { 'publisher':'scientific.net',\n",
    "        'pub_website':'www.scientific.net',\n",
    "        'x_title':'//div[@class=\"paper-title\"]/div[@class=\"paper-name\"]/text()',\n",
    "        'x_abstract':'//div[@class=\"abstract\"]/p/text()',\n",
    "        'x_people':'//div[text()=\"\\r\\n                                        Authors\\r\\n                                    \"]/following-sibling::div/a',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//none-here',#no affiliations given on page\n",
    "        'x_dept':'//none-here',\n",
    "        'x_date':'//div[text()=\"\\r\\n                                        Online since\\r\\n                                    \"]/following-sibling::div/text()',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date.strip(),'%B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record10 = { 'publisher':'jstage',\n",
    "        'pub_website':'www.jstage.jst.go.jp',\n",
    "        'x_title':'string(//*[contains(@class,\"mod-article-heading\")])',\n",
    "        'x_abstract':'string(//*[contains(@class,\"mod-section\")]/p)',\n",
    "        'x_people':'//*[contains(@class,\"author\")]/a',\n",
    "        'x_person':'text()',\n",
    "        'x_depts':'//*[contains(@class,\"affiliation\")]',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'string(//*[contains(@class,\"date\")])',\n",
    "        'date_con':'''\n",
    "ds = re.sub(\"[^0-9]\", \"\", date)\n",
    "dates = [datetime.strptime(ds[i:i+8],'%Y%m%d') for i in range(0,len(ds),8)]\n",
    "date = min(dates)''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record11 = { 'publisher':'rsc',\n",
    "        'pub_website':'pubs.rsc.org',\n",
    "        'x_title':'string(//div[@class=\"article_chemsoc_txt_s13\"])',\n",
    "        'x_abstract':'string(//div[@class=\"abstract_new\"])',\n",
    "        'x_people':'//div[@class=\"peptide_middle\"]/div[1]/span',\n",
    "        'x_person':'a/text()',\n",
    "        'x_depts':'//div[@class=\"show_affiliation_section\"]/div[position()>2]',\n",
    "        'x_dept':'div[2]/text()',\n",
    "        'x_date':'//div[@class=\"peptide_middle\"]/span[last()]/text()[1]',\n",
    "        'date_con':'''\n",
    "try:\n",
    "    d = [i.strip() for i in date.strip().split(' ')]\n",
    "    p = filter(lambda x: not(x==u''),d)\n",
    "    date = u' '.join(p[-3:])\n",
    "    date = datetime.strptime(date,'%d %b %Y')\n",
    "except:\n",
    "    pass''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record12 = { 'publisher':'iop',\n",
    "        'pub_website':'iopscience.iop.org',\n",
    "        'x_title':'string(//*[@class=\"wd-jnl-art-title\"])',\n",
    "        'x_abstract':'string(//div[contains(@class,\"wd-jnl-art-abstract\")]/p)',\n",
    "        'x_people':'//span[@data-authors]/span',\n",
    "        'x_person':'span[@itemprop=\"name\"]/text()',\n",
    "        'x_depts':'//div[@class=\"wd-jnl-art-author-affiliations\"]/p',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//div[contains(@class,\"wd-jnl-art-dates\")]/p/text()',\n",
    "        'date_con':'''\n",
    "d= date.strip()\n",
    "datetime.strptime(' '.join(d.split(u' ')[-3:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record13 = { 'publisher':'RoyalSociety',\n",
    "        'pub_website':'rsif.royalsocietypublishing.org',\n",
    "        'x_title':'//*[@id=\"page-title\"]/text()',\n",
    "        'x_abstract':'//*[@id=\"abstract-1\"]/p/text()',\n",
    "        'x_people':'//span[@class=\"highwire-citation-authors\"]/span',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//span[@class=\"nlm-aff\"]',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'//span[contains(@class,\"highwire-cite-metadata-date\")]/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(' '.join(date.split(u' ')[-3:]),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record14 = { 'publisher':'RoyalSociety',\n",
    "        'pub_website':'rspa.royalsocietypublishing.org',\n",
    "        'x_title':'//*[@id=\"page-title\"]/text()',\n",
    "        'x_abstract':'//*[@id=\"abstract-1\"]/p/text()',\n",
    "        'x_people':'//span[@class=\"highwire-citation-authors\"]/span',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//span[@class=\"nlm-aff\"]',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'//span[contains(@class,\"highwire-cite-metadata-date\")]/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(' '.join(date.split(u' ')[-3:]),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record15 = { 'publisher':'Scitation',\n",
    "        'pub_website':'scitation.aip.org',\n",
    "        'x_title':'string(//*[@class=\"title-with-crossmark\"])',\n",
    "        'x_abstract':'string(//*[contains(@class,\"abstract \")]/descendant::*[@class=\"articleabstract\"]/p[2])',\n",
    "        'x_people':'//span[contains(@class,\"authors\")]/a',\n",
    "        'x_person':'text()',\n",
    "        'x_depts':'//div[contains(@class,\"affiliations\")]/a',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//div[contains(@class,\"itemCitation\")]/span[3]/text()',\n",
    "        'date_con':'''\n",
    "d = date[:-1].split(u\" \")\n",
    "date=datetime.strptime(u' '.join(d[2:4] + [d[-1]]),\"%b %d %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record16 = { 'publisher':'Nature',\n",
    "        'pub_website':'www.nature.com',\n",
    "        'x_title':'//*[@class=\"article-heading\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div/p)',\n",
    "        'x_people':'//*[contains(@class,\"authors citation-authors\")]/li',\n",
    "        'x_person':'a[@class=\"name\"]/span/text()',\n",
    "        'x_depts':'//ol[contains(@class,\"affiliations\")]/li',\n",
    "        'x_dept':'h3/text()',\n",
    "        'x_date':'//*[contains(@class,\"citation dates\")]/dd[1]/time/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(date.strip(),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record17 = { 'publisher':'degruyter',\n",
    "        'pub_website':'www.degruyter.com',\n",
    "        'x_title':'string(//*[@class=\"entryTitle\"])',\n",
    "        'x_abstract':'string(//*[@class=\"articleBody_abstract\"]/p)',\n",
    "        'x_people':'//*[@class=\"contributors\"]/descendant-or-self::*',\n",
    "        'x_person':'./text()',\n",
    "        'x_depts':'//*[contains(@class,\"NLM_affiliations\")]/p',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//*[contains(@id,\"date-received\")]/dd/text()',\n",
    "        'date_con':'''\n",
    "datetime.strptime(date,\"%Y-%m-%d\")''',   \n",
    "        'name_con':'''\n",
    "pex2=[]\n",
    "for p in pex:\n",
    "    if type(p)==list:\n",
    "        for p1 in p:\n",
    "            if (len(p1.strip())>3): \n",
    "                pex2.append(p1.strip())\n",
    "    elif p==[]:\n",
    "        pass\n",
    "    else:\n",
    "        if len(p.strip())>3: \n",
    "            pex2.append(p.strip())\n",
    "pex3=[]\n",
    "for p in pex2:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    p = re.sub('/',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex3.append(name)\n",
    "pex=pex3'''\n",
    "    }\n",
    "\n",
    "\n",
    "paths_list = [record1,record2,record3,record4,record5,record6,record7,record8,record9,record10,record11,record12,record13,record14,record15,record16,record17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'X Zhang',\n",
       " u'CA Paddon',\n",
       " u'Y Chan',\n",
       " u'PC Bulman-Page',\n",
       " u'PS Fordred',\n",
       " u'SD Bull',\n",
       " u'H Chang',\n",
       " u'N Rizvi',\n",
       " u'F Marken']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = response.xpath(paths['x_people'])\n",
    "pex=[]\n",
    "for person in people:\n",
    "    p = person.xpath('text()').extract()\n",
    "    if not(p==[])and (len(p)==1):\n",
    "        pex.append(p[0])\n",
    "    else:\n",
    "        pex.append(p)\n",
    "pex2 = []\n",
    "pexl=[]\n",
    "for p in pex:\n",
    "    if type(p)==list:\n",
    "        pexl+=p\n",
    "    else:\n",
    "        pexl.append(p)\n",
    "for p in pexl:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    p = re.sub(' and','',p )\n",
    "    words = p.split(\" \")\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex2=filter(lambda x: x!=u' ',pex2)\n",
    "pex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article first published online: 10 JUN 2009\n",
      "Voltammetric Antioxidant Analysis in Mineral Oil Samples Immobilized into Boron-Doped Diamond Micropore Array Electrodes\n",
      "********************\n",
      "Mineral oil microdroplets containing the model antioxidant N,N-didodecyl-N′,N′-diethyl-phenylene-diamine (DDPD) are immobilized into a 100×100 pore-array (ca. 10 μm individual pore diameter, 100 μm pitch) in a boron-doped diamond electrode surface. The robust diamond surface allows pore filling, cleaning, and reuse without damage to the electrode surface. The electrode is immersed into aqueous electrolyte media, and voltammetric responses for the oxidation of DDPD are obtained. In order to further improve the current responses, 20 wt% of carbon nanofibers are co-deposited with the oil into the pore array. Voltammetric signals are consistent with the oxidation of DDPD and the associated transfer of perchlorate anions (in aqueous 0.1 M NaClO4) or the transfer of protons (in aqueous 0.1 M HClO4). From the magnitude of the current response, the DDPD content in the mineral oil can be determined down to less than 1 wt% levels. Perhaps surprisingly, the reversible (or midpoint) potential for the DDPD oxidation in mineral oil (when immersed in 0.1 NaClO4) is shown to be concentration-dependent and to shift to more positive potential values for more dilute DDPD in mineral oil solutions. An extraction mechanism and the formation of a separate organic product phase are proposed to explain this behavior.\n",
      "********************\n",
      "PC Bulman-Page\n",
      "H Chang\n",
      "PS Fordred\n",
      "Y Chan\n",
      "N Rizvi\n",
      "CA Paddon\n",
      "X Zhang\n",
      "SD Bull\n",
      "F Marken\n",
      "********************\n",
      "Coordination Chemistry Laboratory, Division of Chemistry, Graduate School of Science, Hokkaido University, Kita-ku, Sapporo, Hokkaido, 060-0810, Japan\n",
      "Laser Micromachining Ltd., OpTIC Technium, St. Asaph Business Park, Denbighshire LL17 0JD, UK\n",
      "Lubricants Technology, Shell Global Solutions, Shell Technology Centre Thornton, Chester CH1 3SH, UK\n",
      "Department of Chemistry, University of Bath, Bath BA2 7AY, UK\n",
      "School of Chemical Sciences & Pharmacy, University of East Anglia, Norwich, Norfolk NR4 7TJ, UK\n",
      "********************\n",
      "2009-06-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "paths_list = [record2]\n",
    "ind = 1\n",
    "for paths in paths_list:\n",
    "    title= response.xpath(paths['x_title']).extract()[0]\n",
    "    abstract = response.xpath(paths['x_abstract']).extract()[0]\n",
    "    people = response.xpath(paths['x_people'])\n",
    "    depts = response.xpath(paths['x_depts'])\n",
    "    pex=[]\n",
    "    dex=[]\n",
    "    for person in people:\n",
    "        p = person.xpath(paths['x_person']).extract()\n",
    "        if not(p==[])and (len(p)==1):\n",
    "            pex.append(p[0])\n",
    "        else:\n",
    "            pex.append(p)\n",
    "    for dept in depts:\n",
    "        d = dept.xpath(paths['x_dept']).extract()\n",
    "        if not(d==[]):\n",
    "            dex.append(d[0])\n",
    "    exec paths['name_con']\n",
    "    pex=list(set(pex))\n",
    "    date = response.xpath(paths['x_date']).extract()[0]\n",
    "    print(date)\n",
    "    exec paths['date_con']\n",
    "    dex = list(set(dex))\n",
    "    item = {\n",
    "        'title':title,\n",
    "        'authors':pex,\n",
    "        'depts':dex,\n",
    "        'abstract':abstract,\n",
    "        'date': date\n",
    "         }\n",
    "    ind+=1\n",
    "print(item['title'])\n",
    "print('*'*20)\n",
    "print(item['abstract'])\n",
    "print('*'*20)\n",
    "for p in item['authors']:\n",
    "    print(p)\n",
    "print('*'*20)\n",
    "for d in item['depts']:\n",
    "    print(d)\n",
    "print('*'*20)\n",
    "print(item['date'])\n",
    "#TO DO: CLEAN THE NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', gaierror(8, 'nodename nor servname provided, or not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c73fe35f09d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget_stub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://dx.doi.org/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_stub\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdoi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m###SENSE Who's publisher this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# By explicitly closing the session, we avoid leaving sockets open which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# can trigger a ResourceWarning in some cases, and look like a memory leak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    463\u001b[0m         }\n\u001b[1;32m    464\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', gaierror(8, 'nodename nor servname provided, or not known'))"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dead=0\n",
    "with open('stuff.json','r') as f:\n",
    "    j = json.load(f)\n",
    "    for rec in j:\n",
    "        item = {}\n",
    "        doi = rec['doi']\n",
    "        target_stub = 'http://dx.doi.org/'\n",
    "        target = target_stub + doi\n",
    "        r = requests.get(target)\n",
    "        response=TextResponse(r.url,body=r.text, encoding='utf-8')\n",
    "        ###SENSE Who's publisher this is\n",
    "        pub = get_publisher(response)\n",
    "        print(pub)\n",
    "        ###LOAD CORRECT XPATHS:\n",
    "        #paths_list = get_paths(pub,ca,doi)\n",
    "        die = True\n",
    "        for rec in paths_list:\n",
    "            if rec['pub_website']==pub:\n",
    "                die=False\n",
    "        if die:\n",
    "            dead+=1\n",
    "            print('I Died with DOI '+str(doi)+\"and publisher\" + str(pub))\n",
    "        else: \n",
    "            ind = 1\n",
    "            for paths in paths_list:\n",
    "                try:\n",
    "                    title= response.xpath(paths['x_title']).extract()[0]\n",
    "                    abstract = response.xpath(paths['x_abstract']).extract()[0]\n",
    "                    people = response.xpath(paths['x_people'])\n",
    "                    depts = response.xpath(paths['x_depts'])\n",
    "                    pex=[]\n",
    "                    dex=[]\n",
    "                    for person in people:\n",
    "                        p = person.xpath(paths['x_person']).extract()\n",
    "                        if not(p==[])and (len(p)==1):\n",
    "                            pex.append(p[0])\n",
    "                        else:\n",
    "                            pex.append(p)\n",
    "                    for dept in depts:\n",
    "                        d = dept.xpath(paths['x_dept']).extract()\n",
    "                        if not(d==[]):\n",
    "                            dex.append(d[0])\n",
    "                    exec paths['name_con']\n",
    "                    pex = list(set(pex))\n",
    "                    date = response.xpath(paths['x_date']).extract()[0]\n",
    "                    exec paths['date_con']\n",
    "                    dex = list(set(dex))\n",
    "                    item = {\n",
    "                        'title':title,\n",
    "                        'authors':pex,\n",
    "                        'depts':dex,\n",
    "                        'abstract':abstract,\n",
    "                        'date': date,\n",
    "                        'doi':doi,\n",
    "                        'publisher':pub\n",
    "                     }\n",
    "                except:\n",
    "                    #print(\"format {0} fail\".format(ind))\n",
    "                    ind+=1\n",
    "                    pass\n",
    "            try:\n",
    "                print(item['title'])\n",
    "                print('*'*20)\n",
    "                print(item['abstract'][0:10])\n",
    "                print('*'*20)\n",
    "                print(item['authors'])\n",
    "                print('*'*20)\n",
    "                print(item['depts'])\n",
    "                print('*'*20)\n",
    "                print(item['date'])\n",
    "            except:\n",
    "                print('I Died with DOI '+str(doi)+\"and publisher\" + str(pub))\n",
    "                print(item)\n",
    "print('proportion of records not collected: ' + str(dead))\n",
    "#TO DO: CLEAN THE NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Michael', u'C.', u'Willis', u'Dr.']\n",
      "[u'Michael', u'C.', u'Willis']\n",
      "[u'Gary', u'A.', u'Cutting']\n",
      "[u'Gary', u'A.', u'Cutting']\n",
      "[u'', u'Vincent', u'J.-D.', u'Piccio']\n",
      "[u'', u'Vincent', u'J.-D.', u'Piccio']\n",
      "[u'Matthew', u'J.', u'Durbin']\n",
      "[u'Matthew', u'J.', u'Durbin']\n",
      "[u'Dr.', u'Matthew', u'P.', u'John']\n",
      "[u'Matthew', u'P.', u'John']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'MC Willis', u'GA Cutting', u'VJ Piccio', u'MJ Durbin', u'MP John']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    print(words)\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    print(words)\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<type 'exceptions.ValueError'>\n",
      "exported\n",
      "1\n",
      "exported\n",
      "2\n",
      "<type 'exceptions.ValueError'>\n",
      "exported\n",
      "3\n",
      "<type 'exceptions.ValueError'>\n",
      "exported\n",
      "4\n",
      "exported\n",
      "5\n",
      "<type 'exceptions.ValueError'>\n",
      "exported\n",
      "6\n",
      "exported\n",
      "7\n",
      "exported\n",
      "8\n",
      "exported\n",
      "proportion of records not collected: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import signal\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "def get_paths(publisher,conn,doi):\n",
    "    ###to do, build database with all of these\n",
    "    cur = ca.find({'pub_website':publisher})\n",
    "    if cur.count()==0:\n",
    "        raise Exception('no publisher website for '+ doi) \n",
    "    return cur\n",
    "\n",
    "def get_publisher(response):\n",
    "    url = response.url\n",
    "    publisher = url.split('/')[2]\n",
    "    return publisher\n",
    "\n",
    "def handler(signum,frame):\n",
    "    raise Exception()\n",
    "\n",
    "def choose_items(items):\n",
    "    if len(items)==1:\n",
    "        return items[0]\n",
    "    else:\n",
    "        scores=[]\n",
    "        for it in items:\n",
    "            score = 0\n",
    "            for k,v in it.items():\n",
    "                v=v.strip()\n",
    "                if not(v==u''): score+=1\n",
    "            scores.append(score)\n",
    "        print(scores)\n",
    "        return items[scores.index(max(scores))]\n",
    "        \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#mongo_url = 'mongodb://localhost:27017/' #local\n",
    "mongo_url = 'mongodb://localhost:6666/' #remote\n",
    "db = 'Cherry'\n",
    "coll = 'CherryMunch'\n",
    "client = MongoClient(mongo_url)\n",
    "ca = client[db][coll]\n",
    "fo = open('scraped.json','w')\n",
    "fd = open('losses.json','w')\n",
    "\n",
    "ind=0\n",
    "dead=0\n",
    "with open('stuff.json','r') as f:\n",
    "    j = json.load(f)\n",
    "    for rec in j[1:10]:\n",
    "        print(ind)\n",
    "        ind+=1\n",
    "        item = {}\n",
    "        doi = rec['doi']\n",
    "        target_stub = 'http://dx.doi.org/'\n",
    "        target = target_stub + doi\n",
    "        die = True\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(10)\n",
    "        try: \n",
    "            r = requests.get(target)\n",
    "            response=TextResponse(r.url,body=r.text, encoding='utf-8')\n",
    "        ###SENSE Who's publisher this is\n",
    "            pub = get_publisher(response)\n",
    "        ###LOAD CORRECT XPATHS:\n",
    "            try:\n",
    "                paths_list = get_paths(pub,ca,doi)\n",
    "                die=False\n",
    "            except:\n",
    "                print(sys.exc_info()[0])\n",
    "                die=True \n",
    "            if die:\n",
    "                print('death')\n",
    "                json.dump({'doi':doi,'error':'missing_pub','pub':pub},fd,sort_keys=True,indent=4,ensure_ascii=True)\n",
    "        except:\n",
    "            print(sys.exc_info()[0])\n",
    "            json.dump({'doi':doi,'error':'timeout'},fd,sort_keys=True,indent=4,ensure_ascii=True)\n",
    "        signal.alarm(0)\n",
    "        if die:\n",
    "            dead+=1\n",
    "        else: \n",
    "            items = []\n",
    "            for paths in paths_list:\n",
    "                try:\n",
    "                    title= response.xpath(paths['x_title']).extract()[0]\n",
    "                    abstract = response.xpath(paths['x_abstract']).extract()[0]\n",
    "                    people = response.xpath(paths['x_people'])\n",
    "                    depts = response.xpath(paths['x_depts'])\n",
    "                    pex=[]\n",
    "                    dex=[]\n",
    "                    for person in people:\n",
    "                        p = person.xpath(paths['x_person']).extract()\n",
    "                        if not(p==[])and (len(p)==1):\n",
    "                            pex.append(p[0])\n",
    "                        else:\n",
    "                            pex.append(p)\n",
    "                    for dept in depts:\n",
    "                        d = dept.xpath(paths['x_dept']).extract()\n",
    "                        if not(d==[]):\n",
    "                            dex.append(d[0])\n",
    "                    exec paths['name_con']\n",
    "                    date = response.xpath(paths['x_date']).extract()[0]\n",
    "                    exec paths['date_con']\n",
    "                    dex = list(set(dex))\n",
    "                    item = {\n",
    "                        'title':title,\n",
    "                        'authors':pex,\n",
    "                        'depts':dex,\n",
    "                        'abstract':abstract,\n",
    "                        'date': date.strftime('%d %B %Y'),\n",
    "                        'doi':doi,\n",
    "                        'publisher':pub\n",
    "                    }\n",
    "                    items.append(item)\n",
    "                except:\n",
    "                    print(sys.exc_info()[0])\n",
    "                    pass\n",
    "            try:\n",
    "                item = choose_items(items)\n",
    "                json.dump(item,fo,sort_keys=True,indent=4,ensure_ascii=True)\n",
    "                print('exported')\n",
    "            except:\n",
    "                print(sys.exc_info()[0])\n",
    "                json.dump({'doi':doi,'error':'collection','pub':pub},fd,sort_keys=True,indent=4,ensure_ascii=False)\n",
    "                dead+=1\n",
    "print('proportion of records not collected: ' + str(dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo.close()\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo = open('scraped.json','w')\n",
    "fd = open('losses.json','w')\n",
    "json.dump({'doi':doi,'error':'collection','pub':pub},fd,sort_keys=True,indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
