{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CherryMunch Prototype\n",
    "\n",
    "Take A DOI, get the page, then collect the relevant data from the page, save and leave\n",
    "WITHOUT interacting with the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record1 = {\n",
    "        'publisher':'Wiley',\n",
    "        'pub_website':'onlinelibrary.wiley.com',\n",
    "        'x_title':'string(//*[@class=\"article-header__title\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div/p)',\n",
    "        'x_people':'//*[@class=\"article-header__authors-item\"]',\n",
    "        'x_depts':'//*[@class=\"article-header__authors-item\"]',\n",
    "        'x_person':'string(.//*[@class=\"article-header__authors-name\"])',\n",
    "        'x_dept':'string(.//*[@class=\"article-header__authors-item-aff-addr\"])',\n",
    "        'x_date':'string(//time[@id=\"first-published-date\"])',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date,'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record2 = {\n",
    "        'publisher':'Wiley',\n",
    "        'pub_website':'onlinelibrary.wiley.com',\n",
    "        'x_title':'string(//*[@class=\"articleTitle\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div[@class=\"para\"])',\n",
    "        'x_people':'//*[@id=\"authors\"]/li',\n",
    "        'x_depts':'//*[@id=\"authorsAffiliations\"]/li',\n",
    "        'x_person':'text()',\n",
    "        'x_dept':'p/text()',\n",
    "        'x_date':'string(//p[@id=\"publishedOnlineDate\"])',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(u' '.join(date.split(' ')[-3:]),'%d %b %Y')\n",
    "        ''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "pexl=[]\n",
    "for p in pex:\n",
    "    if type(p)==list:\n",
    "        pexl+=p\n",
    "    else:\n",
    "        pexl.append(p)\n",
    "for p in pexl:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    p = re.sub(' and','',p )\n",
    "    words = p.split(\" \")\n",
    "    if u'Dr.' in words:\n",
    "        words.remove(u'Dr.')\n",
    "    for w in words[:-1]:\n",
    "        if not(w==u''):\n",
    "            name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=filter(lambda x: x!=u' ',pex2)\n",
    "'''\n",
    "    }\n",
    "\n",
    "record3 = {\n",
    "        'publisher':'ACS',\n",
    "        'pub_website':'pubs.acs.org',\n",
    "        'x_title':'string(//*[@class=\"articleTitle\"])',\n",
    "        'x_abstract':'string(//*[@id=\"abstractBox\"])',\n",
    "        'x_people':'//*[@id=\"authors\"]/span',\n",
    "        'x_person':'string(span[1])',\n",
    "        'x_depts':'//*[@class=\"affiliations\"]/div',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'string(//*[@id=\"pubDate\"])',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(' '.join([sp[-3]]+[sp[-2][:-1]]+[sp[-1]]),'%B %d %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record4 = { 'publisher':'American Physical Society',\n",
    "        'pub_website':'journals.aps.org',\n",
    "        'x_title':'//div[@id=\"title\"]/descendant::h3/text()',\n",
    "        'x_abstract':'//meta[@name=\"description\"]/@content',\n",
    "        'x_people':'//section[@class=\"article authors open\"]/div/p',\n",
    "        'x_person':'*[1]/text()',\n",
    "        'x_depts':'//section[@class=\"article authors open\"]/div/ul[@class=\"no-bullet\"]',\n",
    "        'x_dept':'li/text()',\n",
    "        'x_date':'//ul[@class=\"inline-list pub-dates\"]/li/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(' '.join(sp[1:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record5 = { 'publisher':'Springer',\n",
    "        'pub_website':'link.springer.com',\n",
    "        'x_title':'//*[@class=\"ArticleTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"Abs1\"]/p)',\n",
    "        'x_people':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_person':'*/span[@class=\"AuthorName\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_dept':'descendant::span[@class=\"AuthorsName_affiliation\"]/span/text()',\n",
    "        'x_date':'//*[@class=\"ArticleCitation_Year\"]/time/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(date,'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record6 = { 'publisher':'Springer',\n",
    "        'pub_website':'link.springer.com',\n",
    "        'x_title':'//*[@class=\"ArticleTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"Abs1\"]/p)',\n",
    "        'x_people':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_person':'*/span[@class=\"AuthorName\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"AuthorNames\"]/li',\n",
    "        'x_dept':'descendant::span[@class=\"AuthorsName_affiliation\"]/span/text()',\n",
    "        'x_date':'//*[@class=\"ArticleCitation_Year\"]/time/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(date,'%B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record7 = { 'publisher':'ScienceDirect',\n",
    "        'pub_website':'www.sciencedirect.com',\n",
    "        'x_title':'//*[@class=\"svTitle\"]/text()',\n",
    "        'x_abstract':'string(//*[@class=\"abstract svAbstract \"])',\n",
    "        'x_people':'//ul[@class=\"authorGroup noCollab svAuthor\"]/li',\n",
    "        'x_person':'*[@class=\"authorName svAuthor\"]/text()',\n",
    "        'x_depts':'//ul[@class=\"affiliation authAffil\"]/li',\n",
    "        'x_dept':'span/text()',\n",
    "        'x_date':'//dl[@class=\"articleDates\"]/dd/text()',\n",
    "        'date_con':'''\n",
    "sp = date.split(' ')\n",
    "date = datetime.strptime(\" \".join(sp[-3:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record8 = { 'publisher':'International Union of Crystallography',\n",
    "        'pub_website':'scripts.iucr.org',\n",
    "        'x_title':'string(//*[@class=\"ica_title\"])',\n",
    "        'x_abstract':'string(//*[@class=\"ica_abstract\"])',\n",
    "        'x_people':'//*[@class=\"ica_authors\"]/a',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//none-here',#no affiliations given on page\n",
    "        'x_dept':'//none-here',\n",
    "        'x_date':'//div[@class=\"ica_header\"]/span[2]/text()',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date[2:6],'%Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record9 = { 'publisher':'scientific.net',\n",
    "        'pub_website':'www.scientific.net',\n",
    "        'x_title':'//div[@class=\"paper-title\"]/div[@class=\"paper-name\"]/text()',\n",
    "        'x_abstract':'//div[@class=\"abstract\"]/p/text()',\n",
    "        'x_people':'//div[text()=\"\\r\\n                                        Authors\\r\\n                                    \"]/following-sibling::div/a',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//none-here',#no affiliations given on page\n",
    "        'x_dept':'//none-here',\n",
    "        'x_date':'//div[text()=\"\\r\\n                                        Online since\\r\\n                                    \"]/following-sibling::div/text()',\n",
    "        'date_con':'''\n",
    "date = datetime.strptime(date.strip(),'%B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record10 = { 'publisher':'jstage',\n",
    "        'pub_website':'www.jstage.jst.go.jp',\n",
    "        'x_title':'string(//*[contains(@class,\"mod-article-heading\")])',\n",
    "        'x_abstract':'string(//*[contains(@class,\"mod-section\")]/p)',\n",
    "        'x_people':'//*[contains(@class,\"author\")]/a',\n",
    "        'x_person':'text()',\n",
    "        'x_depts':'//*[contains(@class,\"affiliation\")]',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'string(//*[contains(@class,\"date\")])',\n",
    "        'date_con':'''\n",
    "ds = re.sub(\"[^0-9]\", \"\", date)\n",
    "dates = [datetime.strptime(ds[i:i+8],'%Y%m%d') for i in range(0,len(ds),8)]\n",
    "date = min(dates)''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record11 = { 'publisher':'rsc',\n",
    "        'pub_website':'pubs.rsc.org',\n",
    "        'x_title':'string(//div[@class=\"article_chemsoc_txt_s13\"])',\n",
    "        'x_abstract':'string(//div[@class=\"abstract_new\"])',\n",
    "        'x_people':'//div[@class=\"peptide_middle\"]/div[1]/span',\n",
    "        'x_person':'a/text()',\n",
    "        'x_depts':'//div[@class=\"show_affiliation_section\"]/div[position()>2]',\n",
    "        'x_dept':'div[2]/text()',\n",
    "        'x_date':'//div[@class=\"peptide_middle\"]/span[last()]/text()[1]',\n",
    "        'date_con':'''\n",
    "try:\n",
    "    d = [i.strip() for i in date.strip().split(' ')]\n",
    "    p = filter(lambda x: not(x==u''),d)\n",
    "    date = u' '.join(p[-3:])\n",
    "    date = datetime.strptime(date,'%d %b %Y')\n",
    "except:\n",
    "    pass''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record12 = { 'publisher':'iop',\n",
    "        'pub_website':'iopscience.iop.org',\n",
    "        'x_title':'string(//*[@class=\"wd-jnl-art-title\"])',\n",
    "        'x_abstract':'string(//div[contains(@class,\"wd-jnl-art-abstract\")]/p)',\n",
    "        'x_people':'//span[@data-authors]/span',\n",
    "        'x_person':'span[@itemprop=\"name\"]/text()',\n",
    "        'x_depts':'//div[@class=\"wd-jnl-art-author-affiliations\"]/p',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//div[contains(@class,\"wd-jnl-art-dates\")]/p/text()',\n",
    "        'date_con':'''\n",
    "d= date.strip()\n",
    "datetime.strptime(' '.join(d.split(u' ')[-3:]),'%d %B %Y')''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record13 = { 'publisher':'RoyalSociety',\n",
    "        'pub_website':'rsif.royalsocietypublishing.org',\n",
    "        'x_title':'//*[@id=\"page-title\"]/text()',\n",
    "        'x_abstract':'//*[@id=\"abstract-1\"]/p/text()',\n",
    "        'x_people':'//span[@class=\"highwire-citation-authors\"]/span',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//span[@class=\"nlm-aff\"]',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'//span[contains(@class,\"highwire-cite-metadata-date\")]/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(' '.join(date.split(u' ')[-3:]),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record14 = { 'publisher':'RoyalSociety',\n",
    "        'pub_website':'rspa.royalsocietypublishing.org',\n",
    "        'x_title':'//*[@id=\"page-title\"]/text()',\n",
    "        'x_abstract':'//*[@id=\"abstract-1\"]/p/text()',\n",
    "        'x_people':'//span[@class=\"highwire-citation-authors\"]/span',\n",
    "        'x_person':'string(.)',\n",
    "        'x_depts':'//span[@class=\"nlm-aff\"]',\n",
    "        'x_dept':'string(.)',\n",
    "        'x_date':'//span[contains(@class,\"highwire-cite-metadata-date\")]/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(' '.join(date.split(u' ')[-3:]),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record15 = { 'publisher':'Scitation',\n",
    "        'pub_website':'scitation.aip.org',\n",
    "        'x_title':'string(//*[@class=\"title-with-crossmark\"])',\n",
    "        'x_abstract':'string(//*[contains(@class,\"abstract \")]/descendant::*[@class=\"articleabstract\"]/p[2])',\n",
    "        'x_people':'//span[contains(@class,\"authors\")]/a',\n",
    "        'x_person':'text()',\n",
    "        'x_depts':'//div[contains(@class,\"affiliations\")]/a',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//div[contains(@class,\"itemCitation\")]/span[3]/text()',\n",
    "        'date_con':'''\n",
    "d = date[:-1].split(u\" \")\n",
    "date=datetime.strptime(u' '.join(d[2:4] + [d[-1]]),\"%b %d %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record16 = { 'publisher':'Nature',\n",
    "        'pub_website':'www.nature.com',\n",
    "        'x_title':'//*[@class=\"article-heading\"]/text()',\n",
    "        'x_abstract':'string(//*[@id=\"abstract\"]/div/p)',\n",
    "        'x_people':'//*[contains(@class,\"authors citation-authors\")]/li',\n",
    "        'x_person':'a[@class=\"name\"]/span/text()',\n",
    "        'x_depts':'//ol[contains(@class,\"affiliations\")]/li',\n",
    "        'x_dept':'h3/text()',\n",
    "        'x_date':'//*[contains(@class,\"citation dates\")]/dd[1]/time/text()',\n",
    "        'date_con':'''\n",
    "date=datetime.strptime(date.strip(),\"%d %B %Y\")''',   \n",
    "        'name_con':'''\n",
    "pex2 = []\n",
    "for p in pex:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex2.append(name)\n",
    "pex=pex2'''\n",
    "    }\n",
    "\n",
    "record17 = { 'publisher':'degruyter',\n",
    "        'pub_website':'www.degruyter.com',\n",
    "        'x_title':'string(//*[@class=\"entryTitle\"])',\n",
    "        'x_abstract':'string(//*[@class=\"articleBody_abstract\"]/p)',\n",
    "        'x_people':'//*[@class=\"contributors\"]/descendant-or-self::*',\n",
    "        'x_person':'./text()',\n",
    "        'x_depts':'//*[contains(@class,\"NLM_affiliations\")]/p',\n",
    "        'x_dept':'text()',\n",
    "        'x_date':'//*[contains(@id,\"date-received\")]/dd/text()',\n",
    "        'date_con':'''\n",
    "datetime.strptime(date,\"%Y-%m-%d\")''',   \n",
    "        'name_con':'''\n",
    "pex2=[]\n",
    "for p in pex:\n",
    "    if type(p)==list:\n",
    "        for p1 in p:\n",
    "            if (len(p1.strip())>3): \n",
    "                pex2.append(p1.strip())\n",
    "    elif p==[]:\n",
    "        pass\n",
    "    else:\n",
    "        if len(p.strip())>3: \n",
    "            pex2.append(p.strip())\n",
    "pex3=[]\n",
    "for p in pex2:\n",
    "    name = \"\"\n",
    "    p = re.sub(r\"\\s+\", u\" \", p, flags=re.UNICODE)\n",
    "    p = re.sub(',',u'',p)\n",
    "    p = re.sub('/',u'',p)\n",
    "    words = p.split(\" \")\n",
    "    words = filter(lambda x: not(x==u''),words)\n",
    "    if words[0] in [u'Dr.']:\n",
    "        words.remove(words[0])\n",
    "    for w in words[:-1]:\n",
    "        name+=w[0]\n",
    "    name+=u\" \"+words[-1]\n",
    "    pex3.append(name)\n",
    "pex=pex3'''\n",
    "    }\n",
    "\n",
    "\n",
    "paths_list = [record1,record2,record3,record4,record5,record6,record7,record8,record9,record10,record11,record12,record13,record14,record15,record16,record17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-289-6b748bcd9568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m                         \u001b[0mdex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;32mexec\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name_con'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;32mexec\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_con'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mpex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import signal\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "def get_paths(publisher,conn,doi):\n",
    "    ###to do, build database with all of these\n",
    "    cur = ca.find({'pub_website':publisher})\n",
    "    if cur.count()==0:\n",
    "        raise Exception('no publisher website for '+ doi) \n",
    "    return cur\n",
    "\n",
    "def get_publisher(response):\n",
    "    url = response.url\n",
    "    publisher = url.split('/')[2]\n",
    "    return publisher\n",
    "\n",
    "def handler(signum,frame):\n",
    "    raise Exception()\n",
    "\n",
    "def choose_items(items):\n",
    "    if len(items)==1:\n",
    "        return items[0]\n",
    "    else:\n",
    "        scores=[]\n",
    "        for it in items:\n",
    "            score = 0\n",
    "            for k,v in it.items():\n",
    "                if type(v)=='str':\n",
    "                    v=v.strip()\n",
    "                    if not(v==u''): score+=1\n",
    "                else:\n",
    "                    if not(v==[]):\n",
    "                        score+=1\n",
    "            scores.append(score)\n",
    "        print(scores)\n",
    "        return items[scores.index(max(scores))]\n",
    "\n",
    "mongo_url = 'mongodb://localhost:27017/' #local\n",
    "#mongo_url = 'mongodb://localhost:6666/' #remote\n",
    "db = 'Cherry'\n",
    "coll = 'CherryMunch'\n",
    "client = MongoClient(mongo_url)\n",
    "ca = client[db][coll]\n",
    "ind=0\n",
    "dead=0\n",
    "with open('stuff.json','r') as f:\n",
    "    j = json.load(f)\n",
    "    for rec in j[0:1]:\n",
    "        print(ind)\n",
    "        ind+=1\n",
    "        item = {}\n",
    "        #doi = '10.1515/mgmc-2012-0023'\n",
    "        doi = '10.1039/9781847557612'\n",
    "        target_stub = 'http://dx.doi.org/'\n",
    "        target = target_stub + doi\n",
    "        die = True\n",
    "        r = requests.get(target)\n",
    "        response=TextResponse(r.url,body=r.text, encoding='utf-8')\n",
    "        ###SENSE Who's publisher this is\n",
    "        pub = get_publisher(response)\n",
    "        ###LOAD CORRECT XPATHS:\n",
    "        paths_list = get_paths(pub,ca,doi)\n",
    "        if False:\n",
    "            pass\n",
    "        else: \n",
    "            items = []\n",
    "            for paths in paths_list:\n",
    "                title= response.xpath(paths['x_title']).extract()[0]\n",
    "                abstract = response.xpath(paths['x_abstract']).extract()[0]\n",
    "                people = response.xpath(paths['x_people'])\n",
    "                depts = response.xpath(paths['x_depts'])\n",
    "                pex=[]\n",
    "                dex=[]\n",
    "                for person in people:\n",
    "                    p = person.xpath(paths['x_person']).extract()\n",
    "                    if not(p==[])and (len(p)==1):\n",
    "                        pex.append(p[0])\n",
    "                    else:\n",
    "                        pex.append(p)\n",
    "                for dept in depts:\n",
    "                    d = dept.xpath(paths['x_dept']).extract()\n",
    "                    if not(d==[]):\n",
    "                        dex.append(d[0])\n",
    "                exec paths['name_con']\n",
    "                date = response.xpath(paths['x_date']).extract()[0]\n",
    "                exec paths['date_con']\n",
    "                pex= list(set(pex))\n",
    "                dex = list(set(dex))\n",
    "                item = {\n",
    "                    'title':title,\n",
    "                    'authors':pex,\n",
    "                    'depts':dex,\n",
    "                    'abstract':abstract,\n",
    "                    'date': date.strftime('%d %B %Y'),\n",
    "                    'doi':doi,\n",
    "                    'publisher':pub\n",
    "                }\n",
    "                items.append(item)\n",
    "            item = choose_items(items)\n",
    "            print(item)\n",
    "\n",
    "\n",
    "print('proportion of records not collected: ' + str(dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
