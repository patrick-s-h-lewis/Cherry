{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis suite\n",
    "program to analyse collection run quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "Analysis of scraping run performed at 11:15 30 November 2015\n",
      "**********************************************************************\n",
      "\n",
      "Total records parsed:                          155\n",
      "Total records successfully collected:          17\n",
      "Conversion rate:                               10.9677%\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Losses Breakdown:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Total No. of records not converted:            138\n",
      "No. lost due to errors in collection:          29\n",
      "No. lost due to request timeouts:              2\n",
      "No. lost due to missing publisher info:        107\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Collection Errors Breakdown:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Most collection errors for publisher:           www.nature.com\n",
      "\n",
      "\n",
      "Collection Errors for onlinelibrary.wiley.com  1\n",
      "Collection Errors for www.sciencedirect.com    1\n",
      "Collection Errors for rsif.royalsocietypublishing.org1\n",
      "Collection Errors for www.degruyter.com        5\n",
      "Collection Errors for www.nature.com           6\n",
      "Collection Errors for pubs.acs.org             6\n",
      "Collection Errors for iopscience.iop.org       3\n",
      "Collection Errors for link.springer.com        1\n",
      "Collection Errors for pubs.rsc.org             5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import datetime\n",
    "scraped = 'scraped5'+'.json'\n",
    "failed = 'losses5'+'.json'\n",
    "with open(scraped,'r') as sr:\n",
    "    with open(failed,'r') as fr:\n",
    "        s = json.load(sr)\n",
    "        f = json.load(fr)\n",
    "        coll_no=0\n",
    "        time_no=0\n",
    "        miss_no=0\n",
    "        coll_l=[]\n",
    "        time_l=[]\n",
    "        miss_l=[]\n",
    "        for rec in f:\n",
    "            if rec['error']=='collection':\n",
    "                coll_no+=1\n",
    "                coll_l.append(rec)\n",
    "            if rec['error']=='timeout':\n",
    "                time_no+=1\n",
    "                time_l.append(rec)\n",
    "            if rec['error']=='missing_pub':\n",
    "                miss_no+=1\n",
    "                miss_l.append(rec)\n",
    "\n",
    "success_no=len(s)\n",
    "fail_no=len(f)\n",
    "parse_no = success_no+fail_no\n",
    "conv_no = float(success_no)/float(parse_no)\n",
    "coll_pub = [r['pub'] for r in coll_l]\n",
    "time_pub = [r['pub'] for r in time_l]\n",
    "\n",
    "print('*'*70)\n",
    "print(\"Analysis of scraping run performed at \"+datetime.datetime.now().strftime('%m:%H %d %B %Y'))\n",
    "print('*'*70+'\\n')\n",
    "print('Total records parsed:                          '+ str(parse_no))\n",
    "print('Total records successfully collected:          ' + str(success_no))\n",
    "print('Conversion rate:                               '+'%.4f'%(conv_no*100)+'%')\n",
    "print('\\n')\n",
    "print('-'*70)\n",
    "print('Losses Breakdown:')\n",
    "print('-'*70+'\\n')\n",
    "print('Total No. of records not converted:            '+ str(fail_no))\n",
    "print('No. lost due to errors in collection:          '+ str(coll_no))\n",
    "print('No. lost due to request timeouts:              '+ str(time_no))\n",
    "print('No. lost due to missing publisher info:        '+str(miss_no))\n",
    "print('\\n')\n",
    "print('-'*70)\n",
    "print('Collection Errors Breakdown:')\n",
    "print('-'*70+'\\n')\n",
    "print('Most collection errors for publisher:           '+max(set(coll_pub), key=coll_pub.count))\n",
    "print('\\n')\n",
    "for i in set(coll_pub):\n",
    "    pad = len(i)\n",
    "    print('Collection Errors for '+i+' '*(47-22-pad)+str(coll_pub.count(i)) )\n",
    "print('\\n')\n",
    "print('-'*70)\n",
    "print('Collection Errors Breakdown:')\n",
    "print('-'*70+'\\n')\n",
    "print('Most collection errors for publisher:           '+max(set(coll_pub), key=coll_pub.count))\n",
    "print('\\n')\n",
    "for i in set(coll_pub):\n",
    "    pad = len(i)\n",
    "    print('Collection Errors for '+i+' '*(47-22-pad)+str(coll_pub.count(i)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11:15 30 November 2015'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime('%m:%H %d %B %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
